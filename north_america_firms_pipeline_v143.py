#!/usr/bin/env python3
"""
åŒ—ç±³åœ°åŸŸ NASA FIRMS æ£®æ—ç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ v1.4.3
USAãƒ»ã‚«ãƒŠãƒ€ã‚’ã‚«ãƒãƒ¼ã™ã‚‹åŒ—ç±³ã‚¨ãƒªã‚¢ã®ç«ç½ãƒ‡ãƒ¼ã‚¿åˆ†æã‚·ã‚¹ãƒ†ãƒ 
çµæœã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜

ã‚¨ãƒªã‚¢ç¯„å›²:
- ç·¯åº¦: åŒ—ç·¯25Â°ï½åŒ—ç·¯70Â° (å—éƒ¨USAï½åŒ—æ¥µåœã‚«ãƒŠãƒ€)  
- çµŒåº¦: è¥¿çµŒ170Â°ï½è¥¿çµŒ50Â° (ã‚¢ãƒ©ã‚¹ã‚«ï½æ±æµ·å²¸)
- å¯¾è±¡å›½: ã‚¢ãƒ¡ãƒªã‚«åˆè¡†å›½ã€ã‚«ãƒŠãƒ€
"""

import os
import sys
import json
import logging
import time
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional
from typing import Dict, List, Optional

# ãƒ‘ã‚¹è¨­å®š
scripts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'scripts')
sys.path.append(scripts_dir)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from scripts.data_collector import DataCollector
from scripts.model_loader import ModelLoader
from scripts.embedding_generator import EmbeddingGenerator
from adaptive_clustering_selector import AdaptiveClusteringSelector
from scripts.visualization import VisualizationManager
from cluster_feature_analyzer import ClusterFeatureAnalyzer
from fire_analysis_report_generator import FireAnalysisReportGenerator

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s'
)
logger = logging.getLogger(__name__)


def _time_step(step_name):
    """ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚é–“æ¸¬å®šç”¨ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼"""
    def decorator(func):
        def wrapper(self, *args, **kwargs):
            start_time = time.time()
            logger.info(f"=== Starting: {step_name} ===")
            try:
                result = func(self, *args, **kwargs)
                elapsed = time.time() - start_time
                logger.info(f"âœ… Completed: {step_name} ({elapsed:.2f}s)")
                return result
            except Exception as e:
                elapsed = time.time() - start_time
                logger.error(f"âŒ Failed: {step_name} ({elapsed:.2f}s) - {str(e)}")
                raise
        return wrapper
    return decorator


class NorthAmericaFIRMSPipeline:
    """åŒ—ç±³åœ°åŸŸ FIRMS ç«ç½åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, config_file: str = "config/config_north_america_firms.json"):
        """
        Args:
            config_file: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config_file = config_file
        self.config = self._load_config()
        self.setup_output_directory()
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±è¨ˆ
        self.stats = {
            'start_time': datetime.now(),
            'total_samples': 0,
            'final_samples': 0,
            'processing_steps': [],
            'performance_metrics': {}
        }
        
    def _load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"Configuration loaded: {self.config_file}")
            return config
        except Exception as e:
            logger.error(f"Failed to load config: {e}")
            raise
    
    def setup_output_directory(self):
        """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š"""
        timestamp = datetime.now().strftime("%Y%m%d%H%M")
        base_dir = self.config['adaptive_clustering']['output_dir']
        self.output_dir = f"{base_dir}_{timestamp}"
        
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(f"{self.output_dir}/raw", exist_ok=True)
        os.makedirs(f"{self.output_dir}/cleaned", exist_ok=True)
        
        logger.info(f"Output directory: {self.output_dir}")
        
    @_time_step("Initializing Pipeline Components")
    def _initialize_components(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        logger.info("=== Initializing Pipeline Components ===")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼åˆæœŸåŒ–
        self.model_loader = ModelLoader()
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        model = self.model_loader.load_model()
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå™¨åˆæœŸåŒ–  
        self.embedding_generator = EmbeddingGenerator(
            model=model,
            output_dir=self.output_dir
        )
        
        # å¯è¦–åŒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        self.visualization_manager = VisualizationManager(
            output_dir=self.output_dir
        )
        
        logger.info("All components initialized successfully")
    
    @_time_step("Collecting NASA FIRMS Data (North America Region)")
    def _collect_nasa_firms_data(self) -> Dict:
        """NASA FIRMSãƒ‡ãƒ¼ã‚¿åé›† - åŒ—ç±³åœ°åŸŸ"""
        logger.info("=== Collecting NASA FIRMS Data (North America Region) ===")
        
        # ãƒ‡ãƒ¼ã‚¿åé›†å™¨åˆæœŸåŒ–
        collector = DataCollector()
        
        # åŒ—ç±³ã‚¨ãƒªã‚¢è¨­å®š
        area_params = self.config['nasa_firms']['area_params']
        print(f"Fetching NASA FIRMS data for past {self.config['nasa_firms']['days_back']} days")
        print(f"Area: {area_params}")
        
        # NASA FIRMS API URLæ§‹ç¯‰
        api_url = (
            f"{self.config['nasa_firms']['data_source']}"
            f"{self.config['nasa_firms']['map_key']}/"
            f"{self.config['nasa_firms']['satellite']}/"
            f"{area_params['west']},{area_params['south']},"
            f"{area_params['east']},{area_params['north']}/"
            f"{self.config['nasa_firms']['days_back']}"
        )
        print(f"API URL: {api_url}")
        
        # ãƒ‡ãƒ¼ã‚¿åé›†å®Ÿè¡Œ
        nasa_data = collector.collect_nasa_firms_data(
            area_params=area_params,
            days_back=self.config['nasa_firms']['days_back'],
            map_key=self.config['nasa_firms']['map_key']
        )
        
        if nasa_data is None or len(nasa_data) == 0:
            raise ValueError("No NASA FIRMS data collected")
        
        print(f"Successfully collected {len(nasa_data)} fire detection records")
        
        # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        confidence_threshold = self.config['nasa_firms']['confidence_threshold']
        filtered_data = nasa_data[nasa_data['confidence'] >= confidence_threshold]
        
        print(f"Filtered to {len(filtered_data)} high-confidence detections (>= {confidence_threshold}%)")
        logger.info(f"Filtered to {len(filtered_data)} high-confidence detections (>= {confidence_threshold}%)")
        
        # ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™
        max_samples = self.config['processing']['max_samples']
        if len(filtered_data) > max_samples:
            filtered_data = filtered_data.sample(n=max_samples, random_state=42)
            print(f"Sampled down to {max_samples} records for analysis")
        
        final_data = filtered_data.copy()
        
        logger.info(f"Final dataset: {len(final_data)} NASA FIRMS records for comprehensive analysis")
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        data_file = f"{self.output_dir}/nasa_firms_data.csv"
        final_data.to_csv(data_file, index=False)
        logger.info(f"Data saved: {data_file}")
        
        # çµ±è¨ˆæ›´æ–°
        self.stats['total_samples'] = len(nasa_data)
        self.stats['final_samples'] = len(final_data)
        
        return {
            'nasa_data': final_data,
            'raw_count': len(nasa_data),
            'filtered_count': len(final_data),
            'confidence_threshold': confidence_threshold
        }
    
    @_time_step("Generating Text Embeddings")
    def _generate_embeddings(self, nasa_data) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        logger.info("=== Generating Text Embeddings ===")
        
        # ç«ç½æ¤œå‡ºãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ç”Ÿæˆ
        text_descriptions = []
        for _, row in nasa_data.iterrows():
            desc = (f"Forest fire detected at latitude {row['latitude']:.4f}, "
                   f"longitude {row['longitude']:.4f} with brightness {row['brightness']:.1f}K "
                   f"and {row['confidence']}% confidence on {row['acq_date']} "
                   f"at {str(row['acq_time']).zfill(4)[:2]}:{str(row['acq_time']).zfill(4)[2:]} "
                   f"by {row['satellite']} {row['instrument']}")
            text_descriptions.append(desc)
        
        logger.info(f"Generated {len(text_descriptions)} fire detection text descriptions")
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embeddings, scores = self.embedding_generator.generate_embeddings_batch(text_descriptions)
        
        logger.info(f"Generated {len(embeddings)} embeddings (dim: {embeddings.shape[1]})")
        
        # åŸ‹ã‚è¾¼ã¿ä¿å­˜
        embeddings_file = f"{self.output_dir}/embeddings.npy"
        np.save(embeddings_file, embeddings.cpu().numpy())
        logger.info(f"Embeddings saved: {embeddings_file}")
        
        return {
            'embeddings': embeddings,
            'scores': scores,
            'text_descriptions': text_descriptions
        }
    
    @_time_step("Performing Adaptive Clustering")
    def _perform_clustering(self, embeddings, nasa_data) -> Dict:
        """é©å¿œçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        logger.info("=== Performing Adaptive Clustering ===")
        
        # é©å¿œçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è¨­å®š
        hdbscan_params = self.config['adaptive_clustering']['hdbscan_params']
        kmeans_params = self.config['adaptive_clustering']['kmeans_params']
        
        logger.info(f"Adaptive parameters: HDBSCAN {hdbscan_params}, k-means {kmeans_params}")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é¸æŠå™¨åˆæœŸåŒ–
        clustering_selector = AdaptiveClusteringSelector(
            output_dir=self.output_dir,
            min_cluster_quality=self.config['adaptive_clustering']['quality_thresholds']['min_cluster_quality'],
            max_noise_ratio=self.config['adaptive_clustering']['quality_thresholds']['max_noise_ratio']
        )
        
        # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•é¸æŠãƒ»å®Ÿè¡Œ
        best_result, selection_details = clustering_selector.select_best_clustering(
            embeddings=embeddings,
            hdbscan_params=hdbscan_params,
            kmeans_params=kmeans_params
        )
        
        # çµæœæ¤œè¨¼
        if best_result is None:
            raise ValueError("Clustering failed - no valid results")
        
        labels = best_result.labels
        method_name = best_result.method
        quality_score = best_result.metrics.quality_score
        
        logger.info(f"Selected method: {method_name}")
        logger.info(f"Selection reason: {selection_details.get('selection_reason', 'Not specified')}")
        
        return {
            'labels': labels,
            'method': method_name,
            'quality_score': quality_score,
            'clustering_result': best_result
        }
    
    @_time_step("Creating Comprehensive Visualizations")
    def _create_visualizations(self, embeddings, labels, scores) -> Dict:
        """åŒ…æ‹¬çš„å¯è¦–åŒ–ä½œæˆ"""
        logger.info("=== Creating Comprehensive Visualizations ===")
        
        visualization_files = []
        
        # t-SNEæ¬¡å…ƒå‰Šæ¸›
        coords = self.visualization_manager.reduce_dimensions_tsne(embeddings)
        
        # t-SNEå¯è¦–åŒ–
        tsne_file = self.visualization_manager.create_cluster_plot(
            coords, labels, scores, title="North America FIRMS Fire Analysis Clustering"
        )
        visualization_files.append(tsne_file)
        logger.info(f"âœ… Generated t-SNE plot: {tsne_file}")
        
        # ã‚¹ã‚³ã‚¢åˆ†å¸ƒå¯è¦–åŒ–
        score_file = self.visualization_manager.create_score_distribution_plot(
            scores, labels
        )
        visualization_files.append(score_file)
        logger.info(f"âœ… Generated score distribution: {score_file}")
        
        return {
            'visualization_files': visualization_files,
            'tsne_plot': tsne_file,
            'score_distribution': score_file
        }
    
    @_time_step("Performing Cluster Feature Analysis")
    def _analyze_cluster_features(self, nasa_data, labels, embeddings) -> Dict:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å¾´åˆ†æå®Ÿè¡Œ"""
        logger.info("=== Performing Cluster Feature Analysis ===")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å¾´åˆ†æå™¨åˆæœŸåŒ–
        feature_analyzer = ClusterFeatureAnalyzer(output_dir=self.output_dir)
        
        # åŒ…æ‹¬çš„ç‰¹å¾´åˆ†æå®Ÿè¡Œ
        feature_analysis = feature_analyzer.analyze_cluster_features(
            nasa_data=nasa_data,
            labels=labels,
            embeddings=embeddings
        )
        
        # ç‰¹å¾´åˆ†æå¯è¦–åŒ–ä½œæˆ
        logger.info("Creating feature analysis visualizations...")
        feature_viz_files = feature_analyzer.create_feature_visualizations(
            feature_analysis, self.output_dir
        )
        
        logger.info(f"Created {len(feature_viz_files)} feature visualization files")
        
        # å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤º
        for viz_file in feature_viz_files:
            logger.info(f"  ğŸ“ˆ Feature viz: {viz_file}")
        
        return {
            'feature_analysis': feature_analysis,
            'visualization_files': feature_viz_files
        }
    
    @_time_step("Saving Final Results")
    def _save_results(self, nasa_data, labels, embeddings, clustering_result, 
                     feature_analysis, visualizations) -> Dict:
        """æœ€çµ‚çµæœä¿å­˜"""
        logger.info("=== Saving Final Results ===")
        
        # çµ±è¨ˆè¨ˆç®—
        elapsed_time = (datetime.now() - self.stats['start_time']).total_seconds()
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’è¾æ›¸å½¢å¼ã§ä½œæˆï¼ˆEUç‰ˆã¨åŒæ§˜ï¼‰
        clustering_summary = {
            'quality_score': clustering_result.metrics.quality_score,
            'selected_method': clustering_result.method,
            'method_reason': getattr(clustering_result, 'reason', ''),
            'n_clusters': len(set(labels[labels >= 0])),
            'noise_ratio': (labels == -1).sum() / len(labels)
        }
        
        # æœ€çµ‚çµæœè¾æ›¸ä½œæˆ
        final_results = {
            'timestamp': datetime.now().isoformat(),
            'region': 'North America',
            'total_samples': self.stats['final_samples'],
            'quality_score': clustering_result.metrics.quality_score,  # ãƒ¬ãƒãƒ¼ãƒˆç”¨ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«
            'clustering': clustering_summary,
            'feature_analysis_summary': {
                'total_clusters': len(set(labels[labels >= 0])),
                'geographic_distribution': len(feature_analysis.get('geographic_analysis', {})),
                'temporal_patterns': len(feature_analysis.get('temporal_analysis', {}))
            },
            'config_used': self.config_file
        }
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        results_file = f"{self.output_dir}/final_north_america_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Final results saved: {results_file}")
        
        # ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ä¿å­˜
        labeled_data = nasa_data.copy()
        labeled_data['cluster_label'] = labels
        
        labeled_file = f"{self.output_dir}/north_america_fires_clustered.csv"
        labeled_data.to_csv(labeled_file, index=False)
        logger.info(f"Labeled data saved: {labeled_file}")
        
        return {
            'results_file': results_file,
            'labeled_file': labeled_file,
            'final_results': final_results,
            'processing_time': elapsed_time
        }
    
    @_time_step("Generating Comprehensive Analysis Report")
    def _generate_report(self, nasa_data, labels, clustering_result, feature_analysis) -> str:
        """åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        logger.info("=== Generating Comprehensive Analysis Report ===")
        
        try:
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨åˆæœŸåŒ–ï¼ˆconfigã‚’æ¸¡ã™ï¼‰
            report_generator = FireAnalysisReportGenerator(
                output_dir=self.output_dir, 
                config=self.config
            )
            
            # EUç‰ˆã¨åŒæ§˜ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ä½œæˆ
            clustering_summary = {
                'quality_score': clustering_result.metrics.quality_score,
                'selected_method': clustering_result.method,
                'method_reason': getattr(clustering_result, 'reason', ''),
                'n_clusters': len(set(labels[labels >= 0])),
                'noise_ratio': (labels == -1).sum() / len(labels)
            }
            
            report_data = {
                'data': nasa_data,
                'labels': labels,
                'clustering_results': clustering_summary,
                'feature_analysis': feature_analysis,
                'region': 'North America',
                'region_name': 'North America',
                'focus_country': 'USA/Canada'
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Ÿè¡Œ
            report_file = report_generator.generate_report(report_data)
            
            logger.info(f"ğŸ“ Comprehensive analysis report generated: {report_file}")
            
            return report_file
            
        except Exception as e:
            logger.error(f"Report generation failed: {e}")
            logger.info("Pipeline completed successfully despite report generation issue")
            return None
        
        return report_file
    
    def run_pipeline(self) -> Dict:
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        try:
            logger.info("ğŸ”¥ Starting North America FIRMS Fire Analysis Pipeline")
            
            # 1. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
            self._initialize_components()
            
            # 2. NASA FIRMSãƒ‡ãƒ¼ã‚¿åé›†
            data_result = self._collect_nasa_firms_data()
            nasa_data = data_result['nasa_data']
            
            # 3. åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            embedding_result = self._generate_embeddings(nasa_data)
            embeddings = embedding_result['embeddings']
            scores = embedding_result['scores']
            
            # 4. é©å¿œçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            clustering_result = self._perform_clustering(embeddings, nasa_data)
            labels = clustering_result['labels']
            
            # 5. å¯è¦–åŒ–ä½œæˆ
            visualization_result = self._create_visualizations(embeddings, labels, scores)
            
            # 6. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç‰¹å¾´åˆ†æ
            feature_result = self._analyze_cluster_features(nasa_data, labels, embeddings)
            feature_analysis = feature_result['feature_analysis']
            
            # 7. çµæœä¿å­˜
            save_result = self._save_results(
                nasa_data, labels, embeddings, clustering_result['clustering_result'],
                feature_analysis, visualization_result
            )
            
            # 8. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report_file = self._generate_report(
                nasa_data, labels, clustering_result['clustering_result'], feature_analysis
            )
            
            # æœ€çµ‚çµæœã¾ã¨ã‚
            pipeline_result = {
                'success': True,
                'output_directory': self.output_dir,
                'total_samples': len(nasa_data),
                'processing_time': save_result['processing_time'],
                'clustering_method': clustering_result['method'],
                'quality_score': clustering_result['quality_score'],
                'n_clusters': len(set(labels[labels >= 0])),
                'noise_ratio': (labels == -1).sum() / len(labels),
                'report_file': report_file,
                'results_file': save_result['results_file']
            }
            
            logger.info("ğŸ‰ North America Fire Analysis Pipeline completed successfully!")
            logger.info(f"Processing time: {save_result['processing_time']:.2f}s for {len(nasa_data)} samples")
            
            return pipeline_result
            
        except Exception as e:
            logger.error(f"Pipeline failed: {str(e)}")
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ãƒ»å®Ÿè¡Œ
        pipeline = NorthAmericaFIRMSPipeline()
        result = pipeline.run_pipeline()
        
        # çµæœè¡¨ç¤º
        print("\n" + "="*70)
        print("ğŸ”¥ NORTH AMERICA FOREST FIRE ANALYSIS RESULTS")
        print("="*70)
        print(f"âœ… Status: {'SUCCESS' if result['success'] else 'FAILED'}")
        print(f"ğŸ¯ Selected Method: {result['clustering_method']}")
        print(f"ğŸ“Š Quality Score: {result['quality_score']:.3f}")
        print(f"ğŸ”¢ Clusters Found: {result['n_clusters']}")
        print(f"ğŸ“‰ Noise Ratio: {result['noise_ratio']:.1%}")
        print(f"ğŸ“¦ Total Fire Detections: {result['total_samples']}")
        print(f"â±ï¸ Processing Time: {result['processing_time']:.2f}s")
        print(f"ğŸ“ Results Directory: {result['output_directory']}")
        print(f"ğŸŒ Region Coverage: North America (25Â°N-70Â°N, 170Â°W-50Â°W)")
        print("="*70)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º
        print(f"\nğŸ‰ Analysis completed successfully!")
        print(f"ğŸ“ Results saved in: {result['output_directory']}")
        print(f"ğŸ“„ Main results file: {result['results_file']}")
        
        # ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
        import glob
        output_files = glob.glob(f"{result['output_directory']}/*")
        print(f"ğŸ“Š Generated files ({len(output_files)} total):")
        
        for file_path in sorted(output_files):
            filename = os.path.basename(file_path)
            if filename.endswith('.png'):
                print(f"  ğŸ–¼ï¸  {filename}")
            elif filename.endswith('.md'):
                print(f"  ğŸ“ {filename}", end="")
                if 'comprehensive' in filename:
                    print(" (ğŸ“– COMPREHENSIVE ANALYSIS REPORT)")
                else:
                    print()
            elif filename.endswith('.csv'):
                print(f"  ğŸ“Š {filename}")
            elif filename.endswith('.json'):
                print(f"  ğŸ“‹ {filename}")
            elif filename.endswith('.npy'):
                print(f"  ğŸ“„ {filename}")
            elif os.path.isdir(file_path):
                print(f"  ğŸ“„ {filename}")
        
        # ç‰¹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆæ¡ˆå†…
        if result.get('report_file') and os.path.exists(result['report_file']):
            print(f"\nğŸ“– **åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ**")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {result['report_file']}")
            print(f"   å†…å®¹: 6ã¤ã®å›³è¡¨ã‚’ç”¨ã„ãŸè©³ç´°ãªç«ç½åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
            print(f"   å½¢å¼: Markdownå½¢å¼ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§é–²è¦§å¯èƒ½ï¼‰")
        
        return result
        
    except Exception as e:
        print(f"\nâŒ Pipeline execution failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()